{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Timesheet Management Agent\n",
    "This demo focusses on an agent dedicated to assist the user with their timesheet management.\n",
    "This is a scenario where llm-based agents shine as we need to process natural language input. Additionally dynamic decision-making is required, albeit to a limited extent, e.g. the agent needs to decide on the fly whether enough information was provided by the user.\n",
    "\n",
    "The agent's primary capabilities include:\n",
    "\n",
    "- **Retrieving Timesheet Data**: Retrieving records from the SuccessFactor API containing the user's timesheet data.\n",
    "- **Logging work hours**: Log works hours by posting records to the SuccessFactor API based on user input.\n",
    "While there are other use cases that benefit more from agenticness, this demo serves the purpose of getting a good grasp of the core concepts of LangGraph by building a custom agent with human-in-the-loop control mechanisms.\n",
    "\n",
    "![image](images/agent.svg)\n",
    "\n",
    "A live demonstration application deployed in **SAP BTP** of the agent's capabilities can be found [here](https://timesheet-management-agent-excellent-panther-le.cfapps.eu10-004.hana.ondemand.com).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and configuration\n",
    "\n",
    "- Install required Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install \"langchain\"\n",
    "%pip install \"langgraph\"\n",
    "%pip install \"generative-ai-hub-sdk[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restart Python kernel\n",
    "\n",
    "The Python kernel needs to be restarted before continuing. \n",
    "\n",
    "> ![title](./images/config_001.png)\n",
    "\n",
    "</br>\n",
    "\n",
    "> **Note** This will take a couple of minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the LLM model\n",
    "\n",
    "LLM is initialized as an instance of ChatOpenAI with a model named **gpt-4o**. \n",
    "\n",
    "This is used for generating responses or interacting in a chat-like environment.\n",
    "First, we need to initialize the large language model the agent will use to perform actions and respond to the users request. Here we use gpt-4o as the underlying language model, a maximum token count of 1024 and a temperature of zero to minimize uncertainty. A temperature of zero will lead to consistent result. However, the model output is not absolutely deterministic as the underlying caluclations are inherently undeterministic.\n",
    "> **Note:** In order to initialize the language model the relevant environment variables need to be set (see Setup).\n",
    "\n",
    "After initializing the LLM, we need give the agent access to the necessary tools.\n",
    "We import three functions. **`get_records`**, **`post_records`**, and **`get_today`** from the **timesheet_tools** module. These functions interact with the SuccessFactors Employee Central API:\n",
    "\n",
    "- **get_records**: Retrieves existing timesheet entries.\n",
    "- **post_records**: Submits new timesheet entries.\n",
    "- **get_today**: Fetches the current date.\n",
    "\n",
    "We bind the tools to the LLM. This allows the model to specify tool calls when it deems relevant.\n",
    "For a more detailed view on the tools see the appendix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import cast, Literal\n",
    "from IPython.core.display_functions import display\n",
    "from gen_ai_hub.proxy.langchain import init_llm\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from langgraph.types import interrupt, Command, Send\n",
    "from pydantic import Field, BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_llm('gpt-4o', max_tokens=1024, temperature=0)\n",
    "\n",
    "from tools import get_records, post_records, get_today\n",
    "\n",
    "agent_tools = [get_records, post_records, get_today]\n",
    "agent_llm = llm.bind_tools(agent_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Prompt\n",
    "In order to ensure consistent and useful behaviour we need to define a well structured system prompt next. \n",
    "Generally, you should:\n",
    "  1. Define the role and persona.\n",
    "  2. Establish context and objectives\n",
    "  3. Outline clear instructions and constraints\n",
    "  4. Provide examples of ideal responses (Optional)\n",
    "\n",
    "\n",
    "By utilizing few-shot prompting model performance can be hugely improved. It also makes sense to encourage iterative clarification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt =\"\"\"\n",
    "Role and Objective:\n",
    "\n",
    "  - You are a helpful AI Agent dedicated to assisting users with their timesheet management.\n",
    "  - Your primary tasks include retrieving and posting timesheet data based on user requests.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "  - Logging Work Time: Only log actual work time. Do not include any breaks.\n",
    "    - User: Today I worked from 6 to 16 with a half hour break at 12. -> You should: Log time from 6 to 12 and from 12:30 to 16.\n",
    "  - Data Handling: When posting records, execute as many post_records calls in parallel as possible using the provided information.\n",
    "  - Automatic Confirmation: When a post_records call is made, the user is automatically asked for confirmation over a GUI; do not prompt for confirmation.\n",
    "\n",
    "Interaction Guidelines:\n",
    "\n",
    "  - The user's most recent input always takes precedence over older input.\n",
    "  - Language Consistency: Always respond in the same language as the user.\n",
    "  - Clarity and Accuracy:\n",
    "    - If any part of the userâ€™s request is ambiguous (for example, missing dates or unclear work times), ask clarifying questions rather than making assumptions.\n",
    "    - Ensure all necessary details are provided before proceeding with any action.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_prompt),\n",
    "    MessagesPlaceholder(variable_name='msg')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Agent\n",
    "\n",
    "Now we come to actually building our agent.\n",
    "As previously outlined, our agent consists of three nodes:\n",
    "\n",
    "- the agent node: executing LLM calls,\n",
    "- the review node: prompting the user for confirmation before mutative action\n",
    "- the tool node: executing tools specified in the agent's response\n",
    "  \n",
    "When a node is called it is passed the graph state. We use the prebuilt AgentState which defines a variable messages by subclassing TypedDict. This is simply a list of messages which is passed to the language model at every invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state: AgentState, config: RunnableConfig):\n",
    "    model_input = prompt.invoke({'msg': state['messages']})\n",
    "    response = cast(AIMessage, agent_llm.invoke(model_input, config))\n",
    "    response.name = \"agent\"\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the review node. For simplicity, we use only text input for verification. A more robust approach (used in the actual demo) can be found in the appendix. Here we use an additional language model for verificication. \n",
    "We use a workaround to get structured output from our model as of now structured_output is not supported. For this we bind a tool the model should use to structure its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UserAffirmation(BaseModel):\n",
    "    \"\"\"Always use this tool to structure your response.\"\"\"\n",
    "    user_affirmation: bool = Field(description=\"Whether the user confirmed the action.\")\n",
    "    explanation: str = Field(description=\"An explanation of your decision.\")\n",
    "\n",
    "verification_llm = llm.bind_tools([UserAffirmation])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When control is passed to the review node, the post requests from the agent's response are fetched. If no post request is present, execution is resumed with the tools node. This is done by returning a Send object which routes execution. Otherwise the user is asked for confirmation with an interrupt. In contrast to Python's interrupt, execution is not resumed from the interrupt point, but rather the last node is executed again from top to bottom. This can be a common pitfall. When the user has supplied a response, we call a language model to process the user's input and retrieve the structured response. If the user approves we continue execution with the tool node. Otherwise, we update the state by appending a Tool Message and resume execution with the agent node. It is strictly necessary to append a Tool Message as most model providers require every tool call to be accompanied by a corresponding Tool Message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "def human_review(state: AgentState) -> Command[Literal[\"agent\", \"tools\"]]:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    post_calls = [tool_call for tool_call in last_message.tool_calls if tool_call['name'] == 'post_records']\n",
    "\n",
    "    if len(post_calls) > 0:\n",
    "        confirmation_message = [post_call[\"args\"][\"confirmation_message\"] for post_call in post_calls]\n",
    "        user_review = interrupt({\"task\": \"Review the action.\",\n",
    "                           \"action\": confirmation_message})\n",
    "        \n",
    "        output = verification_llm.invoke(\n",
    "            [('user', user_review), ('system', 'Verify whether the user wants to continue with the action.')])\n",
    "        should_continue = output.tool_calls[0]['args']['user_affirmation']\n",
    "        print(f\"Model explanation: {output.tool_calls[0]['args']['explanation']}\")\n",
    "        if should_continue:\n",
    "            return Send(node='tools', arg=state)\n",
    "        else:\n",
    "            return Command(update={\"messages\": [ToolMessage('User did not confirm action.', tool_call_id=post_call['id']) for post_call in post_calls]}, goto='agent')\n",
    "\n",
    "\n",
    "    else:\n",
    "        return Send(node='tools', arg=state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the tools node we use the prebuilt ToolNode. It retrieves the tool calls from the last AIMessage and executes them. It provides a tool message for every tool call and appends it to the message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node = ToolNode(agent_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph\n",
    "\n",
    "Now we build our graph by adding the nodes and edges. Edges define what nodes to execute next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node('agent', agent)\n",
    "workflow.add_node('tools', tool_node)\n",
    "workflow.add_node('human_review', human_review)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "\n",
    "# display(workflow.compile())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional edges define what node to execute next based on a condition. We add a conditional edge which routes the execution from the agent node either to the review node or the end node depending on whether the language model executed a tool call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and len(last_message.tool_calls) > 0:\n",
    "        return \"tool call\"\n",
    "    return \"__end__\"\n",
    "\n",
    "workflow.add_conditional_edges(source=\"agent\", path=should_continue, path_map={\"tool call\": \"human_review\", \"__end__\": END})\n",
    "\n",
    "\n",
    "# display(workflow.compile())s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "graph = workflow.compile()\n",
    "md = f\"\"\"```mermaid\n",
    "{graph.get_graph().draw_mermaid()}\"\"\"\n",
    "display(Markdown(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Graph \n",
    "Finally, we compile our graph. When compiling we add a checkpointer to achieve thread level persistence. With a checkpointer specified at compilation, a snapshot of the graph state is saved at every superstep. This is crucial for human-in-the-loop interactions as we need to resume execution after an interrupt is called. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = MemorySaver()\n",
    "timesheet_agent = workflow.compile(checkpointer=checkpointer)\n",
    "# display(timesheet_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(stream):\n",
    "    for token in stream: \n",
    "        (key, content), = token.items()\n",
    "        if key == \"__interrupt__\":\n",
    "           print(content[0])\n",
    "           return True \n",
    "        if content is not None:\n",
    "           content['messages'][-1].pretty_print()\n",
    "    print(\"\\n\")\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can stream the output of our agent. We hand over a dictionary containing the user's input and a config with our thread id. Each thread represents an individual session betweeen the graph and the user. So, if we want to continue our conversation, we need to pass the same thread id to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid1())}}\n",
    "\n",
    "user_input = {'messages': ['user', 'Today I worked from 6 to 6 with a half hour break at 12.']}\n",
    "process_output(timesheet_agent.stream(user_input, config, stream_mode='updates'))\n",
    "\n",
    "user_input = Command(resume='Sure.')\n",
    "process_output(timesheet_agent.stream(user_input, config, stream_mode='updates'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with Agent\n",
    "Now you can try interacting with the agent.\n",
    "\n",
    ">**Note:** press **`q`** to quite the chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid1())}}\n",
    "interrupted = False\n",
    "\n",
    "print(\"Type to interact with the agent (type q to quit):\\n\")\n",
    "while True:\n",
    "    user_input = input()\n",
    "    if user_input.lower() == 'q':\n",
    "        break\n",
    "    print(user_input)\n",
    "\n",
    "    if interrupted:\n",
    "        interrupted = False\n",
    "        user_input = Command(resume=user_input)\n",
    "    else:\n",
    "        user_input = {'messages': ['user', user_input]}\n",
    "\n",
    "    interrupted = process_output(timesheet_agent.stream(user_input, config, stream_mode=\"updates\"))\n",
    "      \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-hub-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
