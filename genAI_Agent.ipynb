{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Timesheet Management Agent (Exercise #1)\n",
    "This demo focusses on an agent dedicated to assist the user with their timesheet management.\n",
    "This is a scenario where llm-based agents shine as we need to process natural language input. Additionally dynamic decision-making is required, albeit to a limited extent, e.g. the agent needs to decide on the fly whether enough information was provided by the user.\n",
    "\n",
    "The agent's primary capabilities include:\n",
    "\n",
    "- **Retrieving Timesheet Data**: Retrieving records from the SuccessFactor API containing the user's timesheet data.\n",
    "- **Logging work hours**: Log works hours by posting records to the SuccessFactor API based on user input.\n",
    "While there are other use cases that benefit more from agenticness, this demo serves the purpose of getting a good grasp of the core concepts of LangGraph by building a custom agent with human-in-the-loop control mechanisms.\n",
    "\n",
    "![image](images/agent.svg)\n",
    "\n",
    "A live demonstration application deployed in **SAP BTP** of the agent's capabilities can be found [here](https://timesheet-management-agent-excellent-panther-le.cfapps.eu10-004.hana.ondemand.com).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and configuration\n",
    "\n",
    "- Install required Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install \"langchain>=0.3.0,<0.4.0\"\n",
    "%pip install \"langgraph>=0.2.76,<0.3.0\"\n",
    "%pip install \"langchain-core>=0.3.0,<0.4.0\"\n",
    "%pip install \"langchain-community>=0.3.0,<0.4.0\"\n",
    "%pip install \"sap-ai-sdk-gen[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restart Python kernel\n",
    "\n",
    "The Python kernel needs to be restarted before continuing. \n",
    "\n",
    "> ![title](./images/config_001.png)\n",
    "\n",
    "</br>\n",
    "\n",
    "> **Note** This will take a couple of minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the LLM model\n",
    "\n",
    "LLM is initialized as an instance of ChatOpenAI with a model named **gpt-4o**. \n",
    "\n",
    "This is used for generating responses or interacting in a chat-like environment.\n",
    "First, we need to initialize the large language model the agent will use to perform actions and respond to the users request. Here we use gpt-4o as the underlying language model, a maximum token count of 1024 and a temperature of zero to minimize uncertainty. A temperature of zero will lead to consistent result. However, the model output is not absolutely deterministic as the underlying caluclations are inherently undeterministic.\n",
    "> **Note:** In order to initialize the language model the relevant environment variables need to be set (see Setup).\n",
    "\n",
    "After initializing the LLM, we need give the agent access to the necessary tools.\n",
    "We import three functions. **`get_records`**, **`post_records`**, and **`get_today`** from the **timesheet_tools** module. These functions interact with the SuccessFactors Employee Central API:\n",
    "\n",
    "- **get_records**: Retrieves existing timesheet entries.\n",
    "- **post_records**: Submits new timesheet entries.\n",
    "- **get_today**: Fetches the current date.\n",
    "\n",
    "We bind the tools to the LLM. This allows the model to specify tool calls when it deems relevant.\n",
    "For a more detailed view on the tools see the appendix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import cast, Literal\n",
    "from IPython.core.display_functions import display\n",
    "from gen_ai_hub.proxy.langchain import init_llm\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from langgraph.types import interrupt, Command, Send\n",
    "from pydantic import Field, BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_llm('gpt-4o', max_tokens=1024, temperature=0)\n",
    "\n",
    "from tools import get_records, post_records, get_today\n",
    "\n",
    "agent_tools = [get_records, post_records, get_today]\n",
    "agent_llm = llm.bind_tools(agent_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Prompt\n",
    "In order to ensure consistent and useful behaviour we need to define a well structured system prompt next. \n",
    "Generally, you should:\n",
    "  1. Define the role and persona.\n",
    "  2. Establish context and objectives\n",
    "  3. Outline clear instructions and constraints\n",
    "  4. Provide examples of ideal responses (Optional)\n",
    "\n",
    "\n",
    "By utilizing few-shot prompting model performance can be hugely improved. It also makes sense to encourage iterative clarification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt =\"\"\"\n",
    "Role and Objective:\n",
    "\n",
    "  - You are a helpful AI Agent dedicated to assisting users with their timesheet management.\n",
    "  - Your primary tasks include retrieving and posting timesheet data based on user requests.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "  - Logging Work Time: Only log actual work time. Do not include any breaks.\n",
    "    - User: Today I worked from 6 to 16 with a half hour break at 12. -> You should: Log time from 6 to 12 and from 12:30 to 16.\n",
    "  - Data Handling: When posting records, execute as many post_records calls in parallel as possible using the provided information.\n",
    "  - Automatic Confirmation: When a post_records call is made, the user is automatically asked for confirmation over a GUI; do not prompt for confirmation.\n",
    "\n",
    "Interaction Guidelines:\n",
    "\n",
    "  - The user's most recent input always takes precedence over older input.\n",
    "  - Language Consistency: Always respond in the same language as the user.\n",
    "  - Clarity and Accuracy:\n",
    "    - If any part of the userâ€™s request is ambiguous (for example, missing dates or unclear work times), ask clarifying questions rather than making assumptions.\n",
    "    - Ensure all necessary details are provided before proceeding with any action.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_prompt),\n",
    "    MessagesPlaceholder(variable_name='msg')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Agent\n",
    "\n",
    "Now we come to actually building our agent.\n",
    "As previously outlined, our agent consists of three nodes:\n",
    "\n",
    "- the agent node: executing LLM calls,\n",
    "- the review node: prompting the user for confirmation before mutative action\n",
    "- the tool node: executing tools specified in the agent's response\n",
    "  \n",
    "When a node is called it is passed the graph state. We use the prebuilt AgentState which defines a variable messages by subclassing TypedDict. This is simply a list of messages which is passed to the language model at every invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state: AgentState, config: RunnableConfig):\n",
    "    model_input = prompt.invoke({'msg': state['messages']})\n",
    "    response = cast(AIMessage, agent_llm.invoke(model_input, config))\n",
    "    response.name = \"agent\"\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the review node. For simplicity, we use only text input for verification. A more robust approach (used in the actual demo) can be found in the appendix. Here we use an additional language model for verificication. \n",
    "We use a workaround to get structured output from our model as of now structured_output is not supported. For this we bind a tool the model should use to structure its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UserAffirmation(BaseModel):\n",
    "    \"\"\"Always use this tool to structure your response.\"\"\"\n",
    "    user_affirmation: bool = Field(description=\"Whether the user confirmed the action.\")\n",
    "    explanation: str = Field(description=\"An explanation of your decision.\")\n",
    "\n",
    "verification_llm = llm.bind_tools([UserAffirmation])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When control is passed to the review node, the post requests from the agent's response are fetched. If no post request is present, execution is resumed with the tools node. This is done by returning a Send object which routes execution. Otherwise the user is asked for confirmation with an interrupt. In contrast to Python's interrupt, execution is not resumed from the interrupt point, but rather the last node is executed again from top to bottom. This can be a common pitfall. When the user has supplied a response, we call a language model to process the user's input and retrieve the structured response. If the user approves we continue execution with the tool node. Otherwise, we update the state by appending a Tool Message and resume execution with the agent node. It is strictly necessary to append a Tool Message as most model providers require every tool call to be accompanied by a corresponding Tool Message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "def human_review(state: AgentState) -> Command[Literal[\"agent\", \"tools\"]]:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    post_calls = [tool_call for tool_call in last_message.tool_calls if tool_call['name'] == 'post_records']\n",
    "\n",
    "    if len(post_calls) > 0:\n",
    "        confirmation_message = [post_call[\"args\"][\"confirmation_message\"] for post_call in post_calls]\n",
    "        user_review = interrupt({\"task\": \"Review the action.\",\n",
    "                           \"action\": confirmation_message})\n",
    "        \n",
    "        output = verification_llm.invoke(\n",
    "            [('user', user_review), ('system', 'Verify whether the user wants to continue with the action.')])\n",
    "        should_continue = output.tool_calls[0]['args']['user_affirmation']\n",
    "        print(f\"Model explanation: {output.tool_calls[0]['args']['explanation']}\")\n",
    "        if should_continue:\n",
    "            return Send(node='tools', arg=state)\n",
    "        else:\n",
    "            return Command(update={\"messages\": [ToolMessage('User did not confirm action.', tool_call_id=post_call['id']) for post_call in post_calls]}, goto='agent')\n",
    "\n",
    "\n",
    "    else:\n",
    "        return Send(node='tools', arg=state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the tools node we use the prebuilt ToolNode. It retrieves the tool calls from the last AIMessage and executes them. It provides a tool message for every tool call and appends it to the message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node = ToolNode(agent_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph\n",
    "\n",
    "Now we build our graph by adding the nodes and edges. Edges define what nodes to execute next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node('agent', agent)\n",
    "workflow.add_node('tools', tool_node)\n",
    "workflow.add_node('human_review', human_review)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "\n",
    "display(workflow.compile())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional edges define what node to execute next based on a condition. We add a conditional edge which routes the execution from the agent node either to the review node or the end node depending on whether the language model executed a tool call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"tool_calls\") and len(last_message.tool_calls) > 0:\n",
    "        return \"tool call\"\n",
    "    return \"__end__\"\n",
    "\n",
    "workflow.add_conditional_edges(source=\"agent\", path=should_continue, path_map={\"tool call\": \"human_review\", \"__end__\": END})\n",
    "\n",
    "\n",
    "display(workflow.compile())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Graph \n",
    "Finally, we compile our graph. When compiling we add a checkpointer to achieve thread level persistence. With a checkpointer specified at compilation, a snapshot of the graph state is saved at every superstep. This is crucial for human-in-the-loop interactions as we need to resume execution after an interrupt is called. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = MemorySaver()\n",
    "timesheet_agent = workflow.compile(checkpointer=checkpointer)\n",
    "# display(timesheet_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(stream):\n",
    "    for token in stream: \n",
    "        (key, content), = token.items()\n",
    "        if key == \"__interrupt__\":\n",
    "           print(content[0])\n",
    "           return True \n",
    "        if content is not None:\n",
    "           content['messages'][-1].pretty_print()\n",
    "    print(\"\\n\")\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can stream the output of our agent. We hand over a dictionary containing the user's input and a config with our thread id. Each thread represents an individual session betweeen the graph and the user. So, if we want to continue our conversation, we need to pass the same thread id to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid1())}}\n",
    "\n",
    "user_input = {'messages': ['user', 'Today I worked from 6 to 6 with a half hour break at 12.']}\n",
    "process_output(timesheet_agent.stream(user_input, config, stream_mode='updates'))\n",
    "\n",
    "user_input = Command(resume='Sure.')\n",
    "process_output(timesheet_agent.stream(user_input, config, stream_mode='updates'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with Agent\n",
    "Now you can try interacting with the agent.\n",
    "\n",
    ">**Note:** press **`q`** to quite the chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid1())}}\n",
    "interrupted = False\n",
    "\n",
    "print(\"Type to interact with the agent (type q to quit):\\n\")\n",
    "while True:\n",
    "    user_input = input()\n",
    "    if user_input.lower() == 'q':\n",
    "        break\n",
    "    print(user_input)\n",
    "\n",
    "    if interrupted:\n",
    "        interrupted = False\n",
    "        user_input = Command(resume=user_input)\n",
    "    else:\n",
    "        user_input = {'messages': ['user', user_input]}\n",
    "\n",
    "    interrupted = process_output(timesheet_agent.stream(user_input, config, stream_mode=\"updates\"))\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on SuccessFactors (Exercise #2)\n",
    "\n",
    "Take a look at this linki: https://api.sap.com/api/PLTUserManagement/tryout\n",
    "\n",
    "Now imagine that you has to create an agent for a SuccessFactors API - able to read or write.\n",
    "\n",
    "\n",
    "\n",
    "# Think of it like a restaurant ðŸŽ¯\n",
    "\n",
    "Tool Definition = The kitchen equipment (oven, stove, etc.). These are the capabilities/skills available\n",
    "\n",
    "AI Agent Graph = The chef's brain and training. Decides what to cook, when to use which equipment, in what order\n",
    "\n",
    "User Interaction = The dining room and waiter. Where customers (you) place orders and receive food (answers)\n",
    "\n",
    "\n",
    "Complete flow example:\n",
    "\n",
    "You: \"Show me the top 3 users\"\n",
    "\n",
    "â†“\n",
    "\n",
    "[User Interaction] receives your message\n",
    "\n",
    "â†“\n",
    "\n",
    "[AI Agent Graph] receives it and the Agent thinks:\n",
    "  \"I need to get user data - I should use the get_users tool\"\n",
    "\n",
    "â†“\n",
    "\n",
    "[AI Agent Graph] routes to Tool Node\n",
    "\n",
    "â†“\n",
    "\n",
    "[Tool Definition] executes: get_users(top=3)\n",
    "\n",
    "â†“\n",
    "\n",
    "[Tool Definition] returns user data\n",
    "\n",
    "â†“\n",
    "\n",
    "[AI Agent Graph] Agent thinks again:\n",
    "  \"Great! Now I can format this data for the user\"\n",
    "\n",
    "â†“\n",
    "\n",
    "[AI Agent Graph] creates response\n",
    "\n",
    "â†“\n",
    "\n",
    "[User Interaction] displays the formatted answer to you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import urllib.parse\n",
    "import requests\n",
    "from typing import Annotated, Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOOL DEFINITION ðŸ”§\n",
    "\n",
    "What it does: \n",
    "This section creates a special function that your AI agent can use to fetch user data from an API.\n",
    "\n",
    "Simple explanation:\n",
    "Think of this like giving your AI assistant a specific skill. Just like you might teach someone how to look up information in a phone book, you're teaching the AI how to retrieve user information from a database.\n",
    "\n",
    "Key parts:\n",
    "\n",
    "@tool decorator: This is like putting a label on the function saying \"Hey AI, you can use this!\"\n",
    "\n",
    "get_users(top: int = 20): A function that asks an API for user information. The top parameter controls how many users to retrieve (default is 20).\n",
    "\n",
    "The function handles the technical work: builds a web request, sends it to the API, and returns the data\n",
    "\n",
    "How it fits in:\n",
    "Without tools, your AI can only chat. With tools, it can DO things - like fetching real data. This tool is what allows your AI to go beyond conversation and actually retrieve user information when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. TOOL DEFINITION\n",
    "# ============================================================================\n",
    "\n",
    "API_KEY = 'RIP2DduDl78pmwjhpmJC4p1OUcs2hqbE'\n",
    "\n",
    "get_header = {\n",
    "    'Accept': 'application/json',\n",
    "    'APIKey': API_KEY,\n",
    "    \"DataServiceVersion\": \"2.0\"\n",
    "}\n",
    "\n",
    "base_url = 'https://sandbox.api.sap.com/successfactors/odata/v2/'\n",
    "\n",
    "@tool\n",
    "def get_users(top: int = 20):\n",
    "    \"\"\"Retrieve User entries from the API.\n",
    "    \n",
    "    Args:\n",
    "        top: Specifies the number of entries to retrieve. Defaults to 20.\n",
    "    \n",
    "    Returns:\n",
    "        dict: JSON response data if successful\n",
    "        str: Error message if request fails\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    if top is not None:\n",
    "        params['$top'] = top\n",
    "    \n",
    "    query_text = urllib.parse.urlencode(params, safe='(),')\n",
    "    \n",
    "    table = 'User'\n",
    "    url = f'{base_url}{table}?{query_text}'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=get_header)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data\n",
    "        else:\n",
    "            return f'Error: {response.status_code} - {response.content}'\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f'Exception occurred: {str(e)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI AGENT GRAPH ðŸ¤–\n",
    "\n",
    "What it does:\n",
    "This section builds the \"brain\" of your AI agent - the logic that decides when to chat and when to use tools.\n",
    "\n",
    "Simple explanation:\n",
    "Imagine a flowchart that shows how decisions are made:\n",
    "\n",
    "- User asks a question â†’ AI thinks about it\n",
    "\n",
    "- Does the AI need to use a tool?\n",
    "\n",
    "- YES â†’ Use the tool, then think again about the result\n",
    "\n",
    "- NO â†’ Just respond to the user\n",
    "\n",
    "This is exactly what the \"graph\" does - it's a decision-making flowchart for your AI.\n",
    "\n",
    "\n",
    "Key parts:\n",
    "\n",
    "- StateGraph: The flowchart structure that manages the conversation\n",
    "\n",
    "- call_model node: Where the AI (LLM) thinks and decides what to do\n",
    "\n",
    "- ToolNode: Where tools get executed\n",
    "\n",
    "- should_continue: The decision point - \"Do I need a tool, or am I done?\"\n",
    "\n",
    "- MemorySaver: Remembers the conversation history so the AI doesn't forget what you talked about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. AI AGENT GRAPH\n",
    "# ============================================================================\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    \"\"\"State definition for the agent.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def create_agent():\n",
    "    \"\"\"Creates and compiles the LangGraph agent.\"\"\"\n",
    "    \n",
    "    # Initialize LLM with tool binding\n",
    "    llm = init_llm('gpt-4o', max_tokens=1024, temperature=0)\n",
    "    tools = [get_users]\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    \n",
    "    # Define agent node\n",
    "    def call_model(state: AgentState):\n",
    "        \"\"\"Agent node that calls the LLM.\"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        response = llm_with_tools.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    # Define routing logic\n",
    "    def should_continue(state: AgentState) -> Literal[\"tools\", \"end\"]:\n",
    "        \"\"\"Determines whether to continue to tools or end.\"\"\"\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        \n",
    "        # If there are tool calls, route to tools node\n",
    "        if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "            return \"tools\"\n",
    "        # Otherwise, end\n",
    "        return \"end\"\n",
    "    \n",
    "    # Build the graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"agent\", call_model)\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(START, \"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"tools\": \"tools\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "    \n",
    "    # Compile with memory\n",
    "    memory = MemorySaver()\n",
    "    agent = workflow.compile(checkpointer=memory)\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER INTERACTION ðŸ’¬\n",
    "\n",
    "What it does:\n",
    "This section creates the interface where you (the human) can type messages and see what the AI is doing.\n",
    "\n",
    "Simple explanation:\n",
    "This is like the \"control panel\" where you interact with your AI agent. You type questions, press enter, and see:\n",
    "\n",
    "- What you asked\n",
    "\n",
    "- What the AI is thinking\n",
    "\n",
    "- When it uses tools\n",
    "\n",
    "- What answer it gives you\n",
    "\n",
    "It's a loop that keeps running until you type 'q' to quit.\n",
    "\n",
    "\n",
    "Key parts:\n",
    "\n",
    "main(): Starts everything up and creates a new conversation session\n",
    "\n",
    "while True loop: Keeps the conversation going forever (until you quit)\n",
    "\n",
    "input(\"You: \"): Waits for you to type something\n",
    "\n",
    "agent.stream(): Sends your message to the AI agent and gets responses back\n",
    "\n",
    "process_output(): Takes the AI's response and displays it in a readable format\n",
    "\n",
    "The conversation flow:\n",
    "1. You type: \"Get me 10 users\"\n",
    "2. System shows: [Processing...]\n",
    "3. System shows: Agent is calling get_users tool with top=10\n",
    "4. System shows: Tool returned data\n",
    "5. System shows: Agent's final answer with the user list\n",
    "6. Wait for your next input...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. USER INTERACTION\n",
    "# ============================================================================\n",
    "\n",
    "class Command(TypedDict):\n",
    "    \"\"\"Command structure for resuming interrupted execution.\"\"\"\n",
    "    resume: str\n",
    "\n",
    "\n",
    "def process_output(stream) -> bool:\n",
    "    \"\"\"\n",
    "    Process and display agent output stream.\n",
    "    \n",
    "    Args:\n",
    "        stream: The stream from agent execution\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if execution was interrupted, False otherwise\n",
    "    \"\"\"\n",
    "    interrupted = False\n",
    "    \n",
    "    for event in stream:\n",
    "        for node_name, node_output in event.items():\n",
    "            print(f\"\\n--- {node_name} ---\")\n",
    "            \n",
    "            if \"messages\" in node_output:\n",
    "                for msg in node_output[\"messages\"]:\n",
    "                    if isinstance(msg, AIMessage):\n",
    "                        if msg.content:\n",
    "                            print(f\"Agent: {msg.content}\")\n",
    "                        if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "                            for tool_call in msg.tool_calls:\n",
    "                                print(f\"Tool Call: {tool_call['name']}({tool_call['args']})\")\n",
    "                    elif isinstance(msg, ToolMessage):\n",
    "                        print(f\"Tool Result: {msg.content[:200]}...\")  # Truncate long results\n",
    "                    elif isinstance(msg, HumanMessage):\n",
    "                        print(f\"User: {msg.content}\")\n",
    "            \n",
    "            # Check for interruption (if you implement interrupt logic)\n",
    "            if \"__interrupt__\" in node_output:\n",
    "                interrupted = True\n",
    "                print(\"\\n[Execution interrupted]\")\n",
    "    \n",
    "    return interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Main function to run the interactive agent.\"\"\"\n",
    "    \n",
    "    # Create agent\n",
    "    print(\"Initializing AI Agent...\")\n",
    "    agent = create_agent()\n",
    "    \n",
    "    # Create config with thread ID for memory\n",
    "    config = {\"configurable\": {\"thread_id\": str(uuid.uuid1())}}\n",
    "    interrupted = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AI Agent Ready!\")\n",
    "    print(\"Type your questions to interact with the agent.\")\n",
    "    print(\"The agent can retrieve user information using the get_users tool.\")\n",
    "    print(\"Type 'q' to quit.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'q':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[Processing: {user_input}]\")\n",
    "        \n",
    "        # Prepare input based on interrupt state\n",
    "        if interrupted:\n",
    "            interrupted = False\n",
    "            agent_input = Command(resume=user_input)\n",
    "        else:\n",
    "            agent_input = {\"messages\": [(\"user\", user_input)]}\n",
    "        \n",
    "        # Stream agent execution\n",
    "        try:\n",
    "            interrupted = process_output(\n",
    "                agent.stream(agent_input, config, stream_mode=\"updates\")\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[Error]: {str(e)}\")\n",
    "            interrupted = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
