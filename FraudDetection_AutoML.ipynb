{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAP HANA Cloud - Auto ML Hands On"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "- See **Pypi.org** project for [SAP HANA Python Client API for Machine Learning Algorithms](https://pypi.org/project/hana-ml/) for more information.\n",
    "\n",
    "- For more information on **PAL** see [SAP HANA Predictive Analysis Library (PAL)](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-predictive-analysis-library/sap-hana-cloud-sap-hana-database-predictive-analysis-library-pal?locale=en-US) information page.\n",
    "\n",
    "- See [Python Machine Learning Client for SAP HANA](https://help.sap.com/doc/cd94b08fe2e041c2ba778374572ddba9/2023_2_QRC/en-US/hana_ml.html)to discover **hana-ml** libraries.\n",
    "\n",
    "\n",
    "SAP HANA ML Library\n",
    "You will be using the 'SAP HANA Python Client API for Machine Learning Algorithm'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install hana-ml libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade hana_ml --break-system-packages\n",
    "!pip install --upgrade matplotlib --break-system-packages\n",
    "\n",
    "# Restart Kernel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to your SAP HANA Cloud tenant\n",
    "\n",
    "Define connection details and connect to SAP HANA Cloud tenant.\n",
    "\n",
    "> **Important!** Use username and password supplied via registration e-mail, use hostname for **HANA_TENANT_HOST** supplied in hands-on guide **AutoML Introduction** table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hana_ml.dataframe as dataframe\n",
    "\n",
    "hana_address = 'HANA_TENANT_HOST'\n",
    "hana_port = 443\n",
    "hana_user = 'USERNAME'\n",
    "hana_password = 'PASSWORD'\n",
    "hana_encrypt = True #for HANA Cloud\n",
    "\n",
    "# Establish connection\n",
    "conn = dataframe.ConnectionContext(address = hana_address,\n",
    "                                   port = hana_port, \n",
    "                                   user = hana_user, \n",
    "                                   password = hana_password, \n",
    "                                   encrypt = hana_encrypt,\n",
    "                                   sslValidateCertificate = 'false')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data frame from remote HANA table\n",
    "\n",
    "Data already exists in your schema in table **GX_TRANSACTIONS** . Create a data frame through the SQL or table function and get the row count.\n",
    "\n",
    "> **Important!** Make sure you have successfully created the **GX_TRANSACTIONS** table as instructed in the ***Getting started*** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create data frame\n",
    "df_remote = conn.table(\"GX_TRANSACTIONS\")\n",
    "\n",
    "# Count records in data frame\n",
    "df_remote.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data frame data types\n",
    "\n",
    "Pre-conversion inpection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#control the variable types in SAP HANA\n",
    "df_remote.dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the following variables accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#transform the variable QUALITY\n",
    "df_remote = df_remote.cast('FRAUD', 'NVARCHAR(20)')\n",
    "\n",
    "df_remote = df_remote.cast('AMOUNT', 'DOUBLE')\n",
    "df_remote = df_remote.cast('OLD_BALANCE_ORIGIN', 'DOUBLE')\n",
    "df_remote = df_remote.cast('NEW_BALANCE_ORIGIN', 'DOUBLE')\n",
    "df_remote = df_remote.cast('OLD_BALANCE_DEST', 'DOUBLE')\n",
    "df_remote = df_remote.cast('NEW_BALANCE_DEST', 'DOUBLE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post conversion - take a look at a short description of the data.\n",
    "\n",
    "> **Note:** The target variable is called Fraud. In addition, there are eight predictors capturing different information of a transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#control the variable types\n",
    "df_remote.dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#describe the data in SAP HANA\n",
    "df_remote.describe().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#create training and testing set\n",
    "from hana_ml.algorithms.pal import partition\n",
    "df_remote_train, df_remote_test, df_remote_val = partition.train_test_val_split(data = df_remote, \n",
    "                                                                                   training_percentage = 0.5, \n",
    "                                                                                   testing_percentage = 0.5,\n",
    "                                                                                   validation_percentage = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control the size of the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#control the size of the training and testing set\n",
    "print('Size of training subset: ' + str(df_remote_train.count()))\n",
    "print('Size of test subset: ' + str(df_remote_test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the following dependencies for the Automatic Classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hana_ml import dataframe\n",
    "from hana_ml.dataframe import ConnectionContext\n",
    "from hana_ml.algorithms.pal.utility import DataSets, Settings\n",
    "from hana_ml.algorithms.pal.partition import train_test_val_split\n",
    "from hana_ml.algorithms.pal.auto_ml import AutomaticClassification, AutomaticRegression\n",
    "from hana_ml.visualizers.automl_progress import PipelineProgressStatusMonitor\n",
    "from hana_ml.visualizers.automl_report import BestPipelineReport\n",
    "from hana_ml.visualizers.unified_report import UnifiedReport\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage the workload in SAP HANA Cloud tenant by creating workload classes\n",
    "\n",
    "Workload classes helps us to contol resource utilization in SAP HANA Cloud see [Managing Workload with Workload Classes](https://help.sap.com/docs/SAP_HANA_PLATFORM/6b94445c94ae495c83a19646e7c3fd56/5066181717df4110931271d1efd84cbc.html) for more information.\n",
    "\n",
    "> **Note:** Ignore the error if the work class PAL_AUTOML_WORKLOAD already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn.execute_sql('''\n",
    "CREATE WORKLOAD CLASS \"PAL_AUTOML_WORKLOAD\" SET 'PRIORITY' = '3', 'STATEMENT MEMORY LIMIT' = '3' , 'STATEMENT THREAD LIMIT' = '20'\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the maximum runtime for individual pipeline evaluations with the parameter max_eval_time_mins \n",
    "\n",
    "The AutoML approach automatically executes data processing, model fitting, comparison and optimization.\n",
    "\n",
    "First, create an AutoML classifier object auto_c in the following cell. It is helpful to review and set respective AutoML configuration parameters\n",
    "\n",
    "The defined scenario will run two iterations of pipeline optimization. The total number of pipelines which will be evaluated is equal to population_size + generations Ã— offspring_size. Hence, in this case this amounts to 15 pipelines.\n",
    "With elite_number, you specify how many of the best pipelines you want to compare. Setting random_seed =1234 helps to get reproducable AutoML runs.\n",
    "\n",
    "\n",
    "> **Important!** Change <YourName> to username supplied via **registration e-mail** in the .format() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "scenario_id = \"{}_AutoMLc_{}\".format(\"YOUR_USER_NAME\", uuid.uuid1())\n",
    "print(scenario_id)\n",
    "\n",
    "# Set the initial AutoML scenario parameters\n",
    "auto_c = AutomaticClassification(generations=2, \n",
    "                                 population_size=5,\n",
    "                                 offspring_size=5, \n",
    "                                 elite_number=5,\n",
    "                                 random_seed=1234,\n",
    "                                 progress_indicator_id=scenario_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinitialize and display the AutoML operators and their parameters.\n",
    "\n",
    ">**Note:** A default set of AutoML classification operators and parameters is provided as the global config-dict, which can be adjusted to the needs of the targeted AutoML scenario. Use methods like **update_config_dict, delete_config_dict, display_config_dic** to update the scenario definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reinitialize the AutoML operators and their parameters\n",
    "auto_c.reset_config_dict(conn)\n",
    "auto_c.display_config_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling method choose the SMOTETomek method\n",
    "\n",
    "Adjust some of the settings to narrow the searching space. As the resampling method choose the SMOTETomek method, since the data is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modify the AutoML Classification Scenario\n",
    "\n",
    "# Drop all Resampler\n",
    "auto_c.delete_config_dict(\"SAMPLING\")\n",
    "auto_c.delete_config_dict(\"SMOTE\")\n",
    "auto_c.delete_config_dict(\"TomekLinks\")\n",
    "\n",
    "auto_c.display_config_dict(category=\"Resampler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude the Transformer methods\n",
    "\n",
    "Exclude the Transformer methods. As machine learning algorithms keep the Hybrid Gradient Boosting Tree and Multi Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop and select Transformer\n",
    "auto_c.delete_config_dict(category=\"Transformer\")\n",
    "\n",
    "# Drop and select  Classifier\n",
    "auto_c.delete_config_dict(\"DT_Classifier\")\n",
    "auto_c.delete_config_dict(\"SVM_Classifier\")\n",
    "auto_c.delete_config_dict(\"NB_Classifier\")\n",
    "auto_c.delete_config_dict(\"MLP_Classifier\")\n",
    "auto_c.delete_config_dict(\"RDT_Classifier\")\n",
    "\n",
    "auto_c.display_config_dict(category=\"Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some parameters for the optimization of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change / update Classifier parameter values and ranges\n",
    "auto_c.update_config_dict(\"M_LOGR_Classifier\", \"ENET_LAMBDA\", [0.001, 0.01, 0.1])\n",
    "auto_c.display_config_dict(\"M_LOGR_Classifier\")\n",
    "\n",
    "auto_c.update_config_dict(\"HGBT_Classifier\", \"ETA\", [1e-2, 1e-1, 0.5])\n",
    "auto_c.update_config_dict(\"HGBT_Classifier\", \"MAX_DEPTH\", {'range': [1, 1, 11]})\n",
    "auto_c.update_config_dict(\"HGBT_Classifier\", \"NODE_SIZE\", {'range': [1, 1, 21]})\n",
    "auto_c.display_config_dict(\"HGBT_Classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the complete AutoML configuration for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review complete AutoML Classification configuration\n",
    "auto_c.display_config_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Auto ML scenario on the training data\n",
    "\n",
    "Fit the Auto ML scenario on the training data. It may take a couple of minutes. If it takes to long exclude the SMOTETomek in the resampler() method of the config file.\n",
    "\n",
    "Inspect the pipeline progress through the execution logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# enable_workload_class\n",
    "auto_c.enable_workload_class(workload_class_name=\"PAL_AUTOML_WORKLOAD\")\n",
    "\n",
    "# invoke a PipelineProgressStatusMonitor\n",
    "progress_status_monitor = PipelineProgressStatusMonitor(connection_context= conn, \n",
    "                                                        automatic_obj=auto_c)\n",
    "\n",
    "progress_status_monitor.start()\n",
    "\n",
    "# training\n",
    "try:\n",
    "    auto_c.fit(data=df_remote_train, key='TRANSACTION_ID', label = \"FRAUD\")\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the best model on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = auto_c.model_[1].collect().iat[0, 1]\n",
    "res_ev = auto_c.evaluate(df_remote_test, pipeline=pipeline)\n",
    "print(res_ev.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create predictions with your machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = auto_c.predict(df_remote_test.deselect(\"FRAUD\"), key = 'TRANSACTION_ID')\n",
    "print(res.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best model in SAP HANA\n",
    "\n",
    "> **Important!** Set **YourSchema** to your usename privided in registration e-mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.model_storage import ModelStorage\n",
    "MODEL_SCHEMA = 'TAC004119U01' # HANA schema in which models are to be saved\n",
    "model_storage = ModelStorage(connection_context=conn, schema=MODEL_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model through the following command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_c.name = 'AutoML Classification' \n",
    "auto_c.version = 1\n",
    "model_storage.save_model(model=auto_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "createdAt": "2023-08-01T14:02:38Z",
  "createdBy": "ac42602u01",
  "description": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "modifiedAt": "2023-08-01T14:02:38Z",
  "modifiedBy": "ac42602u01",
  "name": "HANA Cloud Hands On - Fraud Detection Auto ML Script Final.ipynb",
  "scenarioId": "jupyterlab"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
